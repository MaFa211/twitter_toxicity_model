{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from data.data_preprocessing import DefaultENPreprocessor\n",
    "from mb_generator import BalancedMiniBatchLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-08 10:19:00.861909: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2023-06-08 10:19:00.953701: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-08 10:19:01.091213: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-06-08 10:19:01.091753: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-08 10:19:02.330763: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForMaskedLM, Trainer\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"vinai/bertweet-base\")\n",
    "\n",
    "model = AutoModelForMaskedLM.from_pretrained(\"vinai/bertweet-base\")\n",
    "trainer = Trainer(model)\n",
    "trainer.save_model(output_dir = \"/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/LOKAL_DIR/models\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Trainer.data_loader(self) --> data_frame_loader.py.DataFrameLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 56745 before cleaning\n",
      ".... removing \\n and replacing @mentions and URLs by placeholders. Emoji filtering is not done.\n",
      "WARNING you are not removing tweets with media to train a BERT model.\n",
      ".... deleting duplicates\n",
      "Got 54079 after cleaning\n",
      "Making a local copy of Bertweet-base model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n",
      ".... 43264 examples, incl. 43.92% tox in train, 2 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mats/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n",
      "                                                                   \r"
     ]
    }
   ],
   "source": [
    "data = pd.read_csv(\"/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/data/data.csv\", index_col=0)\n",
    "data.columns = [\"label\", \"text\"]\n",
    "\n",
    "prepro = DefaultENPreprocessor()\n",
    "\n",
    "df = prepro(df=data,\n",
    "      label_column='label',\n",
    "      class_weight=0.25,\n",
    "      filter_low_agreements=False,\n",
    "      num_classes=2)\n",
    "\n",
    "#df.to_csv(\"check_preprocess.csv\")\n",
    "\n",
    "mb_loader = BalancedMiniBatchLoader(\n",
    "      fold=\"time\",\n",
    "      seed=1337,\n",
    "      #Should be 0.0 < perc_training_tox < 0.5\n",
    "      perc_training_tox=0.25,\n",
    "      mb_size=64,\n",
    "      n_outer_splits=5,\n",
    "      scope='TOX',\n",
    "      # project=None,\n",
    "      dual_head=False,\n",
    "      sample_weights='class_weight',\n",
    "      huggingface=True,\n",
    "    )\n",
    "\n",
    "#test data = result of split\n",
    "#steps per epoch = simple integer\n",
    "mini_batches, test_data, steps_per_epoch = mb_loader.simple_cv_load(df)\n",
    "\n",
    "#data['tokens'] = data['text'].apply(lambda x: tokenizer.tokenize(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1187"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Making a local copy of Bertweet-base model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "emoji is not installed, thus not converting emoticons or emojis into text. Install emoji: pip3 install emoji==0.6.0\n",
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------- Experiment name:  en2023-05-2212:33:14_defaulttime_Adam_0.01_1_64_0.25_10_seed1337\n",
      "------- Checking there is a GPU\n",
      "Setting up random seed.\n",
      "Loading en data\n",
      "Going to train on everything but the test dataset\n",
      "5\n",
      ".... 45396 examples, incl. 42.56% tox in train, 2 classes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mats/.local/lib/python3.10/site-packages/datasets/arrow_dataset.py:388: FutureWarning: The output of `to_tf_dataset` will change when a passing single element list for `labels` or `columns` in the next datasets version. To return a tuple structure rather than dict, pass a single string.\n",
      "Old behaviour: columns=['a'], labels=['labels'] -> (tf.Tensor, tf.Tensor)  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor)  \n",
      "New behaviour: columns=['a'],labels=['labels'] -> ({'a': tf.Tensor}, {'labels': tf.Tensor})  \n",
      "             : columns='a', labels='labels' -> (tf.Tensor, tf.Tensor) \n",
      "  warnings.warn(\n",
      "                                                                   "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded callback for test, from_logits: True, labels ['']\n",
      ".... using warm-up for 1207 steps\n",
      ".... Adam w global clipnorm None\n",
      "Loading model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'load' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/load_model.ipynb Cell 5\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/load_model.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtrain\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/load_model.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m test \u001b[39m=\u001b[39m Trainer(optimizer_name\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mAdam\u001b[39m\u001b[39m'\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/load_model.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m                weight_decay\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m, \n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/load_model.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m                learning_rate\u001b[39m=\u001b[39m\u001b[39m0.01\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/load_model.ipynb#W4sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m                mb_size\u001b[39m=\u001b[39m\u001b[39m64\u001b[39m,\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/load_model.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m                train_epochs\u001b[39m=\u001b[39m\u001b[39m10\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/mats/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/load_model.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m test\u001b[39m.\u001b[39;49mtrain_full_model(data)\n",
      "File \u001b[0;32m~/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/train.py:407\u001b[0m, in \u001b[0;36mTrainer.train_full_model\u001b[0;34m(self, df)\u001b[0m\n\u001b[1;32m    404\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mGoing to train on everything but the test dataset\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    405\u001b[0m mini_batches, test_data, steps_per_epoch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmb_loader\u001b[39m.\u001b[39msimple_cv_load(df)\n\u001b[0;32m--> 407\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_train_single_fold(\n\u001b[1;32m    408\u001b[0m   mb_generator\u001b[39m=\u001b[39;49mmini_batches, test_data\u001b[39m=\u001b[39;49mtest_data, steps_per_epoch\u001b[39m=\u001b[39;49msteps_per_epoch, fold\u001b[39m=\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mfull\u001b[39;49m\u001b[39m\"\u001b[39;49m\n\u001b[1;32m    409\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/train.py:383\u001b[0m, in \u001b[0;36mTrainer._train_single_fold\u001b[0;34m(self, mb_generator, test_data, steps_per_epoch, fold, val_data)\u001b[0m\n\u001b[1;32m    379\u001b[0m optimizer, callbacks \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_training_actors(\n\u001b[1;32m    380\u001b[0m   steps_per_epoch\u001b[39m=\u001b[39msteps_per_epoch, val_data\u001b[39m=\u001b[39mval_data, test_data\u001b[39m=\u001b[39mtest_data, fold\u001b[39m=\u001b[39mfold\n\u001b[1;32m    381\u001b[0m )\n\u001b[1;32m    382\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mLoading model\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m--> 383\u001b[0m model \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mload_model(optimizer)\n\u001b[1;32m    384\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNb of steps per epoch: \u001b[39m\u001b[39m{\u001b[39;00msteps_per_epoch\u001b[39m}\u001b[39;00m\u001b[39m ---- launching training\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    385\u001b[0m training_args \u001b[39m=\u001b[39m {\n\u001b[1;32m    386\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mepochs\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrain_epochs,\n\u001b[1;32m    387\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39msteps_per_epoch\u001b[39m\u001b[39m\"\u001b[39m: steps_per_epoch,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    390\u001b[0m   \u001b[39m\"\u001b[39m\u001b[39mverbose\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m2\u001b[39m,\n\u001b[1;32m    391\u001b[0m }\n",
      "File \u001b[0;32m~/Documents/University/4_Semester/SDSProjectsSeminar/the-algorithm/trust_and_safety_models/toxicity/train.py:351\u001b[0m, in \u001b[0;36mTrainer.load_model\u001b[0;34m(self, optimizer)\u001b[0m\n\u001b[1;32m    347\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_model\u001b[39m(\u001b[39mself\u001b[39m, optimizer):\n\u001b[1;32m    348\u001b[0m   smart_bias_value \u001b[39m=\u001b[39m (\n\u001b[1;32m    349\u001b[0m     np\u001b[39m.\u001b[39mlog(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperc_training_tox \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mperc_training_tox)) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msmart_bias_init \u001b[39melse\u001b[39;00m \u001b[39m0\u001b[39m\n\u001b[1;32m    350\u001b[0m   )\n\u001b[0;32m--> 351\u001b[0m   model \u001b[39m=\u001b[39m load(\n\u001b[1;32m    352\u001b[0m     optimizer,\n\u001b[1;32m    353\u001b[0m     seed\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseed,\n\u001b[1;32m    354\u001b[0m     trainable\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtrainable,\n\u001b[1;32m    355\u001b[0m     model_type\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_type,\n\u001b[1;32m    356\u001b[0m     loss_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mloss_name,\n\u001b[1;32m    357\u001b[0m     num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnum_classes,\n\u001b[1;32m    358\u001b[0m     additional_layer\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39madditional_layer,\n\u001b[1;32m    359\u001b[0m     smart_bias_value\u001b[39m=\u001b[39msmart_bias_value,\n\u001b[1;32m    360\u001b[0m     content_num_classes\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent_num_classes,\n\u001b[1;32m    361\u001b[0m     content_loss_name\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent_loss_name,\n\u001b[1;32m    362\u001b[0m     content_loss_weight\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontent_loss_weight\n\u001b[1;32m    363\u001b[0m   )\n\u001b[1;32m    365\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_reload \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mFalse\u001b[39;00m:\n\u001b[1;32m    366\u001b[0m     model_folder \u001b[39m=\u001b[39m upload_model(full_gcs_model_path\u001b[39m=\u001b[39mos\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39mjoin(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_dir, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_reload))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load' is not defined"
     ]
    }
   ],
   "source": [
    "from train import Trainer\n",
    "test = Trainer(optimizer_name='Adam', \n",
    "               weight_decay=0.01, \n",
    "               learning_rate=0.01,\n",
    "               mb_size=64,\n",
    "               train_epochs=10)\n",
    "test.train_full_model(data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
